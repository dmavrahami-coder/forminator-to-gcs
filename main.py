from flask import Flask, request, jsonify
from flask_cors import CORS
from google.cloud import storage
import logging
from datetime import datetime
import json
import os
import uuid
from werkzeug.utils import secure_filename
import mimetypes

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

# ============ ×”×’×“×¨×•×ª ============
GCS_BUCKET_NAME = 'aiquantifier-uploads'
ALLOWED_EXTENSIONS = {
    # ×ª××•× ×•×ª
    'png', 'jpg', 'jpeg', 'gif', 'bmp', 'svg', 'webp', 'heic',
    # ××¡××›×™×
    'pdf', 'doc', 'docx', 'xls', 'xlsx', 'ppt', 'pptx', 'odt', 'ods',
    # ×˜×§×¡×˜
    'txt', 'csv', 'json', 'xml', 'html',
    # ××¨×›×™×•× ×™×
    'zip', 'rar', '7z', 'tar', 'gz',
    # ××“×™×”
    'mp4', 'avi', 'mov', 'wmv', 'flv', 'mkv',
    'mp3', 'wav', 'm4a', 'ogg', 'flac'
}

MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
MAX_TOTAL_SIZE = 500 * 1024 * 1024  # 500MB ×œ×›×œ ×¤×¨×•×™×§×˜

# ============ ××ª×—×•×œ ============
storage_client = storage.Client()
bucket = storage_client.bucket(GCS_BUCKET_NAME)

# ×××’×¨ ×–×× ×™ (×‘×¤×¨×•×“×§×©×Ÿ ×ª×©×ª××© ×‘-Firestore/DB)
submissions_db = []
processed_ids = set()

# ============ ×¤×•× ×§×¦×™×•×ª ×¢×–×¨ ============
def allowed_file(filename):
    """×‘×•×“×§ ×× ×¡×•×’ ×”×§×•×‘×¥ ××•×ª×¨"""
    if not filename:
        return False
    if '.' not in filename:
        return False
    ext = filename.rsplit('.', 1)[1].lower()
    return ext in ALLOWED_EXTENSIONS

def generate_project_id():
    """
    ××™×™×¦×¨ ××–×”×” ×¤×¨×•×™×§×˜ ×‘×ª×‘× ×™×ª: YYYYMMDD_HHMMSS
    ×“×•×’××”: 20251207_143025
    """
    return datetime.now().strftime('%Y%m%d_%H%M%S')

def get_file_size(file):
    """××—×–×™×¨ ×’×•×“×œ ×§×•×‘×¥"""
    current_pos = file.tell()
    file.seek(0, 2)  # ×¡×•×£ ×”×§×•×‘×¥
    size = file.tell()
    file.seek(current_pos)  # ×—×–×¨×” ×œ××™×§×•× ×”××§×•×¨×™
    return size

def upload_to_gcs(file_stream, filename, project_id, field_name):
    """××¢×œ×” ×§×•×‘×¥ ×œ-GCS ×•××—×–×™×¨ URL"""
    try:
        # ×‘×“×™×§×ª ×’×•×“×œ
        file_size = get_file_size(file_stream)
        if file_size > MAX_FILE_SIZE:
            raise ValueError(f"File too large: {file_size} bytes (max: {MAX_FILE_SIZE})")
        
        # ×©××•×¨ ×©× ××§×•×¨×™
        original_filename = secure_filename(filename)
        file_extension = original_filename.rsplit('.', 1)[1].lower() if '.' in original_filename else ''
        
        # ×¦×•×¨ ×©× ×§×•×‘×¥ ×™×™×—×•×“×™ ×¢× timestamp
        timestamp = datetime.now().strftime('%H%M%S')
        file_uuid = uuid.uuid4().hex[:6]
        safe_filename = f"{timestamp}_{file_uuid}_{original_filename}"
        
        # × ×ª×™×‘ ××œ× ×‘-GCS: projects/20251207_143025/143025_abc123_filename.jpg
        gcs_path = f"projects/{project_id}/{safe_filename}"
        
        # ××ª×—×•×œ blob
        blob = bucket.blob(gcs_path)
        
        # × ×™×—×•×© content type
        content_type, _ = mimetypes.guess_type(filename)
        if not content_type:
            content_type = 'application/octet-stream'
        
        # ×”×¢×œ××” ×œ-GCS
        file_stream.seek(0)
        blob.upload_from_file(
            file_stream,
            content_type=content_type,
            timeout=600  # 10 ×“×§×•×ª timeout
        )
        
        # ×¦×•×¨ signed URL (×ª×§×£ ×œ-7 ×™××™×)
        url = blob.generate_signed_url(
            version="v4",
            expiration=datetime.timedelta(days=7),
            method="GET"
        )
        
        # ××™×“×¢ ×¢×œ ×”×§×•×‘×¥
        file_info = {
            'field_name': field_name,
            'original_filename': original_filename,
            'gcs_filename': safe_filename,
            'gcs_path': gcs_path,
            'url': url,
            'size': file_size,
            'content_type': content_type,
            'upload_time': datetime.now().isoformat(),
            'bucket': GCS_BUCKET_NAME,
            'project_id': project_id
        }
        
        logging.info(f"âœ… File uploaded: {original_filename} â†’ gs://{GCS_BUCKET_NAME}/{gcs_path}")
        return file_info
        
    except Exception as e:
        logging.error(f"âŒ Error uploading {filename}: {str(e)}")
        raise

# ============ Routes ============
@app.route('/', methods=['GET'])
def home():
    return jsonify({
        'service': 'Forminator to GCS Uploader',
        'version': '2.0',
        'gcs_bucket': GCS_BUCKET_NAME,
        'project_id_format': 'YYYYMMDD_HHMMSS',
        'endpoints': {
            'webhook': '/webhook (POST)',
            'get_files': '/files/<project_id> (GET)',
            'health': '/health (GET)',
            'projects': '/projects (GET)'
        },
        'timestamp': datetime.now().isoformat()
    }), 200

@app.route('/webhook', methods=['POST', 'OPTIONS'])
def handle_forminator_webhook():
    """××§×‘×œ ×§×‘×¦×™× ×•× ×ª×•× ×™× ×-Forminator"""
    
    # CORS preflight
    if request.method == 'OPTIONS':
        return '', 200, {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type'
        }
    
    start_time = datetime.now()
    project_id = None
    uploaded_files = []
    
    try:
        logging.info("=" * 60)
        logging.info("ğŸ“¨ FORMINTOR WEBHOOK - FILE UPLOAD")
        logging.info(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ×™×¦×™×¨×ª project_id ×‘×ª×‘× ×™×ª YYYYMMDD_HHMMSS
        project_id = generate_project_id()
        logging.info(f"ğŸ“‚ PROJECT ID: {project_id}")
        logging.info(f"ğŸ“ GCS Path: projects/{project_id}/")
        
        # ×§×‘×œ×ª form data
        form_data = {}
        if request.form:
            form_data = request.form.to_dict()
            logging.info(f"ğŸ“ Form Data: {json.dumps(form_data, indent=2)}")
        
        # ×§×‘×œ×ª files
        files_data = {}
        if request.files:
            files_data = request.files.to_dict()
            logging.info(f"ğŸ“¦ Files received: {len(files_data)}")
        
        # ×”×¢×œ××ª ×§×‘×¦×™× ×œ-GCS
        if files_data:
            total_size = 0
            file_counter = 1
            
            for field_name, file in files_data.items():
                if file and file.filename and allowed_file(file.filename):
                    file_size = get_file_size(file)
                    total_size += file_size
                    
                    if total_size > MAX_TOTAL_SIZE:
                        raise ValueError(f"Total files size exceeds limit: {total_size} bytes")
                    
                    logging.info(f"  â”Œâ”€â”€ File #{file_counter}: {file.filename}")
                    logging.info(f"  â”œâ”€â”€ Size: {file_size:,} bytes")
                    logging.info(f"  â”œâ”€â”€ Field: {field_name}")
                    
                    # ×”×¢×œ××” ×œ-GCS
                    file_info = upload_to_gcs(
                        file,
                        file.filename,
                        project_id,
                        field_name
                    )
                    
                    if file_info:
                        uploaded_files.append(file_info)
                        logging.info(f"  â””â”€â”€ âœ… Uploaded to: {file_info['gcs_path']}")
                    
                    file_counter += 1
                else:
                    if file and file.filename:
                        logging.warning(f"  âœ— Skipping invalid file: {file.filename}")
                    else:
                        logging.warning("  âœ— Empty file field")
        
        # ×™×¦×™×¨×ª ×¨×©×•××ª submission
        submission_id = f"sub_{project_id}"
        current_time = datetime.now()
        
        submission = {
            'id': submission_id,
            'project_id': project_id,
            'form_data': form_data,
            'files': uploaded_files,
            'files_count': len(uploaded_files),
            'total_size': sum(f['size'] for f in uploaded_files),
            'received_at': current_time.isoformat(),
            'formatted_time': current_time.strftime('%d/%m/%Y %H:%M:%S'),
            'processed': False,
            'form_id': form_data.get('form_id', 'unknown'),
            'entry_id': form_data.get('entry_id', submission_id),
            'gcs_bucket': GCS_BUCKET_NAME,
            'gcs_folder': f"gs://{GCS_BUCKET_NAME}/projects/{project_id}/",
            'gcs_console_url': f"https://console.cloud.google.com/storage/browser/{GCS_BUCKET_NAME}/projects/{project_id}"
        }
        
        # ×©××•×¨ ×‘×××’×¨ ×–×× ×™
        submissions_db.append(submission)
        
        # ×—×™×©×•×‘ ×–××Ÿ ×¢×™×‘×•×“
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # ×¡×™×›×•×
        logging.info("=" * 60)
        logging.info("ğŸ¯ SUBMISSION SUMMARY")
        logging.info(f"   Project ID: {project_id}")
        logging.info(f"   Files Uploaded: {len(uploaded_files)}")
        logging.info(f"   Total Size: {sum(f['size'] for f in uploaded_files):,} bytes")
        logging.info(f"   Processing Time: {processing_time:.2f} seconds")
        logging.info(f"   GCS Location: projects/{project_id}/")
        logging.info(f"   Upload Time: {submission['formatted_time']}")
        logging.info("=" * 60)
        
        # ×ª×©×•×‘×” ×œ-Forminator
        response = {
            'success': True,
            'message': 'Files uploaded successfully to GCS',
            'submission_id': submission_id,
            'project_id': project_id,
            'files_uploaded': len(uploaded_files),
            'total_size': submission['total_size'],
            'gcs_bucket': GCS_BUCKET_NAME,
            'gcs_folder': submission['gcs_folder'],
            'upload_timestamp': submission['received_at'],
            'formatted_time': submission['formatted_time'],
            'project_id_format': 'YYYYMMDD_HHMMSS',
            'processing_time': processing_time,
            'file_list': [
                {
                    'original_name': f['original_filename'],
                    'gcs_path': f['gcs_path'],
                    'size': f['size'],
                    'url': f['url']
                } for f in uploaded_files
            ]
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        logging.error("âŒ" * 20)
        logging.error(f"ERROR PROCESSING WEBHOOK: {str(e)}")
        logging.error(f"Project ID: {project_id}")
        logging.error("âŒ" * 20)
        
        return jsonify({
            'success': False,
            'error': str(e),
            'project_id': project_id,
            'timestamp': datetime.now().isoformat()
        }), 400

@app.route('/get-unprocessed', methods=['GET'])
def get_unprocessed():
    """××—×–×™×¨ ×¨×©×•××•×ª ×©×œ× ×¢×•×‘×“×• ×œ-Apps Script"""
    try:
        limit = int(request.args.get('limit', 100))
        
        unprocessed = [
            s for s in submissions_db 
            if not s['processed'] and s['id'] not in processed_ids
        ]
        
        results = unprocessed[:limit]
        
        return jsonify({
            'success': True,
            'count': len(results),
            'records': results,
            'gcs_bucket': GCS_BUCKET_NAME,
            'timestamp': datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logging.error(f"Error in get-unprocessed: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 400

@app.route('/mark-processed', methods=['POST'])
def mark_processed():
    """×¡×™××•×Ÿ ×¨×©×•××•×ª ×›××¢×•×‘×“×•×ª"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'No data provided'}), 400
        
        submission_ids = data.get('ids', [])
        marked_count = 0
        
        for sub_id in submission_ids:
            processed_ids.add(sub_id)
            for sub in submissions_db:
                if sub['id'] == sub_id:
                    sub['processed'] = True
            marked_count += 1
        
        logging.info(f"Marked {marked_count} submissions as processed")
        
        return jsonify({
            'success': True,
            'marked': marked_count,
            'total_processed': len(processed_ids)
        }), 200
        
    except Exception as e:
        logging.error(f"Error marking as processed: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 400

@app.route('/files/<project_id>', methods=['GET'])
def list_project_files(project_id):
    """××—×–×™×¨ ×¨×©×™××ª ×§×‘×¦×™× ×©×œ ×¤×¨×•×™×§×˜ ×¡×¤×¦×™×¤×™"""
    try:
        # ×—×¤×© ×§×‘×¦×™× ×‘-GCS
        blobs = bucket.list_blobs(prefix=f"projects/{project_id}/")
        
        files = []
        for blob in blobs:
            files.append({
                'name': blob.name.split('/')[-1],
                'path': blob.name,
                'size': blob.size,
                'updated': blob.updated.isoformat(),
                'url': blob.generate_signed_url(
                    version="v4",
                    expiration=datetime.timedelta(days=1),
                    method="GET"
                )
            })
        
        # ×¡×“×¨ ×œ×¤×™ ×–××Ÿ (×”×—×“×©×™× ×¨××©×•×Ÿ)
        files.sort(key=lambda x: x['updated'], reverse=True)
        
        return jsonify({
            'success': True,
            'project_id': project_id,
            'files_count': len(files),
            'total_size': sum(f['size'] for f in files),
            'files': files,
            'gcs_path': f"gs://{GCS_BUCKET_NAME}/projects/{project_id}/",
            'console_url': f"https://console.cloud.google.com/storage/browser/{GCS_BUCKET_NAME}/projects/{project_id}"
        }), 200
        
    except Exception as e:
        logging.error(f"Error listing files: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 400

@app.route('/projects', methods=['GET'])
def list_projects():
    """××—×–×™×¨ ×¨×©×™××ª ×›×œ ×”×¤×¨×•×™×§×˜×™× ×‘-GCS"""
    try:
        # ×—×¤×© ××ª ×›×œ ×”×ª×™×§×™×•×ª projects/
        blobs = bucket.list_blobs(prefix="projects/", delimiter='/')
        
        projects = []
        for prefix in blobs.prefixes:
            project_id = prefix.rstrip('/').split('/')[-1]
            
            # ×—×©×‘ ×›××” ×§×‘×¦×™× ×™×© ×‘×¤×¨×•×™×§×˜
            project_blobs = list(bucket.list_blobs(prefix=prefix))
            
            projects.append({
                'project_id': project_id,
                'files_count': len(project_blobs),
                'total_size': sum(b.size for b in project_blobs),
                'last_modified': max([b.updated for b in project_blobs]).isoformat() if project_blobs else None,
                'gcs_path': f"gs://{GCS_BUCKET_NAME}/{prefix}",
                'console_url': f"https://console.cloud.google.com/storage/browser/{GCS_BUCKET_NAME}/{prefix.rstrip('/')}"
            })
        
        # ×¡×“×¨ ×œ×¤×™ project_id (×”×—×“×©×™× ×¨××©×•×Ÿ)
        projects.sort(key=lambda x: x['project_id'], reverse=True)
        
        return jsonify({
            'success': True,
            'projects_count': len(projects),
            'projects': projects,
            'gcs_bucket': GCS_BUCKET_NAME
        }), 200
        
    except Exception as e:
        logging.error(f"Error listing projects: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 400

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    # ×‘×“×•×§ ×—×™×‘×•×¨ ×œ-GCS
    try:
        bucket.exists()
        gcs_status = 'connected'
    except Exception as e:
        gcs_status = f'error: {str(e)}'
    
    return jsonify({
        'status': 'healthy',
        'service': 'Forminator to GCS Uploader',
        'gcs_bucket': GCS_BUCKET_NAME,
        'gcs_status': gcs_status,
        'submissions_count': len(submissions_db),
        'processed_count': len(processed_ids),
        'project_id_format': 'YYYYMMDD_HHMMSS',
        'timestamp': datetime.now().isoformat(),
        'current_project_id_example': generate_project_id()
    }), 200

if __name__ == '__main__':
    # ×”×’×“×¨ logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    port = int(os.environ.get('PORT', 8080))
    
    logging.info("=" * 60)
    logging.info(f"ğŸš€ Starting Forminator to GCS Uploader")
    logging.info(f"ğŸ“‚ GCS Bucket: {GCS_BUCKET_NAME}")
    logging.info(f"ğŸ†” Project ID Format: YYYYMMDD_HHMMSS")
    logging.info(f"ğŸŒ Port: {port}")
    logging.info("=" * 60)
    
    app.run(host='0.0.0.0', port=port, debug=False)
